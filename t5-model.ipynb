{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7052770,"sourceType":"datasetVersion","datasetId":4059198}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install tensorflow transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-26T20:55:10.571946Z","iopub.execute_input":"2023-11-26T20:55:10.572770Z","iopub.status.idle":"2023-11-26T20:55:27.183367Z","shell.execute_reply.started":"2023-11-26T20:55:10.572727Z","shell.execute_reply":"2023-11-26T20:55:27.181629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\ndata = pd.read_csv(\"/kaggle/input/cleaned-amazon-reviews/your_file.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-26T20:55:27.186254Z","iopub.execute_input":"2023-11-26T20:55:27.186741Z","iopub.status.idle":"2023-11-26T20:55:37.559839Z","shell.execute_reply.started":"2023-11-26T20:55:27.186694Z","shell.execute_reply":"2023-11-26T20:55:37.558524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['t5_input'] = \"summarize: \" + data['cleaned_text']\ndata['t5_output'] = data['cleaned_summary']","metadata":{"execution":{"iopub.status.busy":"2023-11-26T20:55:37.561987Z","iopub.execute_input":"2023-11-26T20:55:37.562502Z","iopub.status.idle":"2023-11-26T20:55:37.960717Z","shell.execute_reply.started":"2023-11-26T20:55:37.562456Z","shell.execute_reply":"2023-11-26T20:55:37.959595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(data['t5_input'], data['t5_output'], test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T20:55:37.963441Z","iopub.execute_input":"2023-11-26T20:55:37.963801Z","iopub.status.idle":"2023-11-26T20:55:38.841474Z","shell.execute_reply.started":"2023-11-26T20:55:37.963770Z","shell.execute_reply":"2023-11-26T20:55:38.840093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5Tokenizer\nimport tensorflow as tf\n\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\nmax_length = 512  # Adjust as needed\nmax_target_length = 150  # Adjust as needed\n\n# Ensure all data is converted to strings and handle any null values\nx_train = x_train.astype(str).fillna('')  # Replace '' with some placeholder text if necessary\ny_train = y_train.astype(str).fillna('')\n\nx_val = x_val.astype(str).fillna('')\ny_val = y_val.astype(str).fillna('')\n\ndef tokenize(inputs, targets, tokenizer, max_length, max_target_length):\n    # Tokenizing the inputs (source text)\n    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n\n    # Preparing decoder_input_ids\n    # For T5, prepend with the pad token\n    pad_token = tokenizer.pad_token_id\n    decoder_input_ids = [[pad_token] + tokenizer.encode(target, add_special_tokens=False) for target in targets]\n    decoder_input_ids = tf.keras.preprocessing.sequence.pad_sequences(decoder_input_ids, maxlen=max_target_length, padding=\"post\", truncating=\"post\")\n\n    # Labels (target text)\n    labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"tf\").input_ids\n    labels = tf.where(labels == pad_token, -100, labels)  # Replace pad_token_id in labels with -100\n\n    return model_inputs, decoder_input_ids, labels\n\nx_train_tokenized, x_train_decoder_input_ids, y_train_tokenized = tokenize(list(x_train), list(y_train), tokenizer, max_length, max_target_length)\nx_val_tokenized, x_val_decoder_input_ids, y_val_tokenized = tokenize(list(x_val), list(y_val), tokenizer, max_length, max_target_length)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T20:55:38.843514Z","iopub.execute_input":"2023-11-26T20:55:38.843913Z","iopub.status.idle":"2023-11-26T21:03:00.689545Z","shell.execute_reply.started":"2023-11-26T20:55:38.843879Z","shell.execute_reply":"2023-11-26T21:03:00.687322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFT5ForConditionalGeneration\n\nmodel = TFT5ForConditionalGeneration.from_pretrained('t5-small')","metadata":{"execution":{"iopub.status.busy":"2023-11-26T21:03:00.691768Z","iopub.execute_input":"2023-11-26T21:03:00.695616Z","iopub.status.idle":"2023-11-26T21:03:12.233229Z","shell.execute_reply.started":"2023-11-26T21:03:00.695565Z","shell.execute_reply":"2023-11-26T21:03:12.231989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"x_train_tokenized['input_ids'].shape:\", x_train_tokenized['input_ids'].shape)\nprint(\"x_trin_decoder_input_ids.shape:\", x_train_decoder_input_ids.shape)\nprint(\"y_train_tokenized.shape:\", y_train_tokenized.shape)\n\nprint(\"Data type of x_train_tokenized['input_ids']:\", x_train_tokenized['input_ids'].dtype)\nprint(\"Data type of x_train_decoder_input_ids:\", x_train_decoder_input_ids.dtype)\nprint(\"Data type of y_train_tokenized:\", y_train_tokenized.dtype)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T21:03:12.238967Z","iopub.execute_input":"2023-11-26T21:03:12.239377Z","iopub.status.idle":"2023-11-26T21:03:12.247225Z","shell.execute_reply.started":"2023-11-26T21:03:12.239343Z","shell.execute_reply":"2023-11-26T21:03:12.246078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Function to check for invalid values in arrays\ndef check_invalid_values(array, name):\n    if np.any(np.isnan(array)):\n        print(f\"NaN values found in {name}\")\n    elif np.any(np.isinf(array)):\n        print(f\"Infinite values found in {name}\")\n    else:\n        print(\"No NaN or inf found\")\n\n# Check for invalid values in the tokenized data\ncheck_invalid_values(x_train_tokenized['input_ids'], \"x_train_tokenized['input_ids']\")\ncheck_invalid_values(x_train_decoder_input_ids, \"x_train_decoder_input_ids\")\ncheck_invalid_values(y_train_tokenized, \"y_train_tokenized\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T21:11:44.063296Z","iopub.execute_input":"2023-11-26T21:11:44.063915Z","iopub.status.idle":"2023-11-26T21:11:44.405986Z","shell.execute_reply.started":"2023-11-26T21:11:44.063836Z","shell.execute_reply":"2023-11-26T21:11:44.404698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nimport numpy as np\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint_callback = ModelCheckpoint(\n    filepath=\"{epoch:02d}-{val_loss:.2f}.h5\",\n    monitor='val_loss',\n    verbose=1,\n    save_best_only=True,\n    save_weights_only=True,\n    mode='min',\n    period=1\n)\n\n# Replace -100 with 0 (or another appropriate value) in label data\ny_train_tokenized = np.where(y_train_tokenized == -100, 0, y_train_tokenized)\ny_val_tokenized = np.where(y_val_tokenized == -100, 0, y_val_tokenized)\n\n# Define a custom loss function if needed\ndef custom_loss(y_true, y_pred):\n    # Create a mask to ignore padding (0) in the labels\n    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n    loss = loss_object(y_true, y_pred)\n    loss *= mask  # Apply the mask to ignore padding\n    return tf.reduce_mean(loss)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n# Compile the model with the custom loss function\nmodel.compile(optimizer=optimizer, loss=custom_loss)\n\n# Train the model\nmodel.fit(\n    {\n        \"input_ids\": x_train_tokenized['input_ids'], \n        \"decoder_input_ids\": x_train_decoder_input_ids, \n        \"labels\": y_train_tokenized\n    }, \n    epochs=3,  \n    batch_size=8,  \n    validation_data=(\n        {\n            \"input_ids\": x_val_tokenized['input_ids'], \n            \"decoder_input_ids\": x_val_decoder_input_ids,\n            \"labels\": y_val_tokenized\n        }\n    ),\n    callbacks=[checkpoint_callback]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T21:19:42.947126Z","iopub.execute_input":"2023-11-26T21:19:42.949413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"t5_model\")","metadata":{},"execution_count":null,"outputs":[]}]}