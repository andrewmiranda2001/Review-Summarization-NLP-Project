## Review-Summarization-NLP-Project

So far, I preprocessed the Amazon Fine Food Reviews dataset and trained a Seq2Seq model. Then I worked on fine tuning the model to produce more optimal summaries for our use case and then experimented with textrank and kmeans clustering with BERT/GPT2 embeddings. I also trained a Text-to-Text transformer using our data, then evaluated pretrained models (BART, T5, and PEGASUS) on summarizing lists of summaries. I then attempted to convert these outputs into 1-2 humanlike sentences by either prompting GPT-2 or using a rule-based POS tagger.

#Update to come:
A refined Seq2Seq model along with experiments training GPT-2 on review generation and then summary generation to attempt to produce a more robust dataset. 
