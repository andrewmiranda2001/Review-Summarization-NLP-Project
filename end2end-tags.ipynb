{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2157,"sourceType":"datasetVersion","datasetId":18},{"sourceId":6645972,"sourceType":"datasetVersion","datasetId":3836404},{"sourceId":7052770,"sourceType":"datasetVersion","datasetId":4059198},{"sourceId":7070177,"sourceType":"datasetVersion","datasetId":4066005},{"sourceId":7115482,"sourceType":"datasetVersion","datasetId":4103481},{"sourceId":153483254,"sourceType":"kernelVersion"}],"dockerImageVersionId":29955,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport re\nfrom bs4 import BeautifulSoup\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport warnings\npd.set_option(\"display.max_colwidth\", 200)\n!pip install openai\n\nwarnings.filterwarnings(\"ignore\")\nfrom shutil import copyfile\ncopyfile(src = \"/kaggle/input/attention-py/attention.py\", dst = \"/kaggle/working/attention.py\")\nfrom attention import AttentionLayer\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:53:06.082422Z","iopub.execute_input":"2023-12-05T17:53:06.082901Z","iopub.status.idle":"2023-12-05T17:53:46.476314Z","shell.execute_reply.started":"2023-12-05T17:53:06.082838Z","shell.execute_reply":"2023-12-05T17:53:46.475201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndata=pd.read_csv(\"/kaggle/input/cleaned-amazon-reviews/your_file.csv\")\ndata","metadata":{"id":"wnK5o4Z1Fxcj","execution":{"iopub.status.busy":"2023-12-05T17:53:46.478478Z","iopub.execute_input":"2023-12-05T17:53:46.478787Z","iopub.status.idle":"2023-12-05T17:53:56.241788Z","shell.execute_reply.started":"2023-12-05T17:53:46.478754Z","shell.execute_reply":"2023-12-05T17:53:56.240345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_text_len=150\nmax_summary_len=12","metadata":{"id":"ZKD5VOWqFxhC","execution":{"iopub.status.busy":"2023-12-05T17:53:56.243538Z","iopub.execute_input":"2023-12-05T17:53:56.243896Z","iopub.status.idle":"2023-12-05T17:53:56.249982Z","shell.execute_reply.started":"2023-12-05T17:53:56.243861Z","shell.execute_reply":"2023-12-05T17:53:56.248736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.replace('', np.nan, inplace=True)\ndata.dropna(axis=0,inplace=True)\ncleaned_text =np.array(data['cleaned_text'])\ncleaned_summary=np.array(data['cleaned_summary'])\n\nshort_text=[]\nshort_summary=[]\n\nfor i in range(len(cleaned_text)):\n    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n        short_text.append(cleaned_text[i])\n        short_summary.append(cleaned_summary[i])\n        \ndf=pd.DataFrame({'text':short_text,'summary':short_summary})\ndf['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')","metadata":{"id":"yY0tEJP0FxhI","execution":{"iopub.status.busy":"2023-12-05T17:53:56.251541Z","iopub.execute_input":"2023-12-05T17:53:56.251985Z","iopub.status.idle":"2023-12-05T17:53:59.210175Z","shell.execute_reply.started":"2023-12-05T17:53:56.251952Z","shell.execute_reply":"2023-12-05T17:53:59.208923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0, shuffle=True) ","metadata":{"id":"RakakKHcFxhl","execution":{"iopub.status.busy":"2023-12-05T17:53:59.213949Z","iopub.execute_input":"2023-12-05T17:53:59.214260Z","iopub.status.idle":"2023-12-05T17:53:59.412112Z","shell.execute_reply.started":"2023-12-05T17:53:59.214229Z","shell.execute_reply":"2023-12-05T17:53:59.410010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_tokenizer = Tokenizer() \nx_tokenizer.fit_on_texts(list(x_tr))","metadata":{"id":"oRHTgX6hFxhq","execution":{"iopub.status.busy":"2023-12-05T17:53:59.415748Z","iopub.execute_input":"2023-12-05T17:53:59.416083Z","iopub.status.idle":"2023-12-05T17:54:20.682890Z","shell.execute_reply.started":"2023-12-05T17:53:59.416050Z","shell.execute_reply":"2023-12-05T17:54:20.681672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresh=2\n\ncnt=0\ntot_cnt=0\nfreq=0\ntot_freq=0\n\nfor key,value in x_tokenizer.word_counts.items():\n    tot_cnt=tot_cnt+1\n    tot_freq=tot_freq+value\n    if(value<thresh):\n        cnt=cnt+1\n        freq=freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq/tot_freq)*100)","metadata":{"id":"y8KronV2Fxhx","outputId":"d2eb2f27-fbbc-4e61-9556-3c3ff5e4327b","execution":{"iopub.status.busy":"2023-12-05T17:54:20.684385Z","iopub.execute_input":"2023-12-05T17:54:20.684897Z","iopub.status.idle":"2023-12-05T17:54:20.761158Z","shell.execute_reply.started":"2023-12-05T17:54:20.684852Z","shell.execute_reply":"2023-12-05T17:54:20.760306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \nx_tokenizer.fit_on_texts(list(x_tr))\n\n#convert text sequences into integer sequences\nx_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \nx_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n\n#padding zero upto maximum length\nx_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\nx_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n\n#size of vocabulary ( +1 for padding token)\nx_voc   =  x_tokenizer.num_words + 1","metadata":{"id":"J2giEsF3Fxh3","execution":{"iopub.status.busy":"2023-12-05T17:54:20.762192Z","iopub.execute_input":"2023-12-05T17:54:20.762454Z","iopub.status.idle":"2023-12-05T17:55:04.308626Z","shell.execute_reply.started":"2023-12-05T17:54:20.762426Z","shell.execute_reply":"2023-12-05T17:55:04.307427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_voc","metadata":{"id":"DCbGMsm4FxiA","outputId":"2d9165f0-e542-4114-91f3-e070d483fce9","execution":{"iopub.status.busy":"2023-12-05T17:55:04.310946Z","iopub.execute_input":"2023-12-05T17:55:04.311343Z","iopub.status.idle":"2023-12-05T17:55:04.317518Z","shell.execute_reply.started":"2023-12-05T17:55:04.311298Z","shell.execute_reply":"2023-12-05T17:55:04.316790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_tokenizer = Tokenizer()\ny_tokenizer.fit_on_texts(list(y_tr))","metadata":{"id":"eRHqyBkBFxiJ","execution":{"iopub.status.busy":"2023-12-05T17:55:04.318644Z","iopub.execute_input":"2023-12-05T17:55:04.319048Z","iopub.status.idle":"2023-12-05T17:55:10.318378Z","shell.execute_reply.started":"2023-12-05T17:55:04.319013Z","shell.execute_reply":"2023-12-05T17:55:10.317332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresh=3\n\ncnt=0\ntot_cnt=0\nfreq=0\ntot_freq=0\n\nfor key,value in y_tokenizer.word_counts.items():\n    tot_cnt=tot_cnt+1\n    tot_freq=tot_freq+value\n    if(value<thresh):\n        cnt=cnt+1\n        freq=freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq/tot_freq)*100)","metadata":{"id":"yzE5OiRLFxiM","outputId":"7f7a4f89-b088-4847-8172-09e5a2383d0e","execution":{"iopub.status.busy":"2023-12-05T17:55:10.319934Z","iopub.execute_input":"2023-12-05T17:55:10.320729Z","iopub.status.idle":"2023-12-05T17:55:10.351738Z","shell.execute_reply.started":"2023-12-05T17:55:10.320624Z","shell.execute_reply":"2023-12-05T17:55:10.350411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare a tokenizer for summaries on training data\ny_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \ny_tokenizer.fit_on_texts(list(y_tr))\n\n#convert text sequences into integer sequences\ny_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \ny_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n\n#padding zero upto maximum length\ny_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\ny_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n\n#size of vocabulary\ny_voc  =   y_tokenizer.num_words +1","metadata":{"id":"-fswLvIgFxiR","execution":{"iopub.status.busy":"2023-12-05T17:55:10.352898Z","iopub.execute_input":"2023-12-05T17:55:10.353313Z","iopub.status.idle":"2023-12-05T17:55:25.108977Z","shell.execute_reply.started":"2023-12-05T17:55:10.353278Z","shell.execute_reply":"2023-12-05T17:55:25.107289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_tokenizer.word_counts['sostok'],len(y_tr)","metadata":{"id":"pR8IX9FRFxiY","outputId":"b116cdbd-42c4-4ede-9f6d-46284115393e","execution":{"iopub.status.busy":"2023-12-05T17:55:25.111077Z","iopub.execute_input":"2023-12-05T17:55:25.111403Z","iopub.status.idle":"2023-12-05T17:55:25.120127Z","shell.execute_reply.started":"2023-12-05T17:55:25.111369Z","shell.execute_reply":"2023-12-05T17:55:25.118165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=[]\nfor i in range(len(y_tr)):\n    cnt=0\n    for j in y_tr[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_tr=np.delete(y_tr,ind, axis=0)\nx_tr=np.delete(x_tr,ind, axis=0)","metadata":{"id":"kZ-vW82sFxih","execution":{"iopub.status.busy":"2023-12-05T17:55:25.122068Z","iopub.execute_input":"2023-12-05T17:55:25.122451Z","iopub.status.idle":"2023-12-05T17:55:34.805953Z","shell.execute_reply.started":"2023-12-05T17:55:25.122412Z","shell.execute_reply":"2023-12-05T17:55:34.804069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=[]\nfor i in range(len(y_val)):\n    cnt=0\n    for j in y_val[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_val=np.delete(y_val,ind, axis=0)\nx_val=np.delete(x_val,ind, axis=0)","metadata":{"id":"cx5NISuMFxik","execution":{"iopub.status.busy":"2023-12-05T17:55:34.808396Z","iopub.execute_input":"2023-12-05T17:55:34.808841Z","iopub.status.idle":"2023-12-05T17:55:35.859126Z","shell.execute_reply.started":"2023-12-05T17:55:34.808794Z","shell.execute_reply":"2023-12-05T17:55:35.858302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K \nK.clear_session()\n\nlatent_dim = 300\nembedding_dim=100\n\n# Encoder\nencoder_inputs = Input(shape=(max_text_len,))\n\n#embedding layer\nenc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n\n#encoder lstm 1\nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.1)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n#encoder lstm 2\nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.1)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n#encoder lstm 3\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.2,recurrent_dropout=0.1)\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\n#embedding layer\ndec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.2,recurrent_dropout=0.1)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n\n# Attention layer\nattn_layer = AttentionLayer(name='attention_layer')\nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n\n# Concat attention input and decoder LSTM output\ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n\n#dense layer\ndecoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_concat_input)\n\n# Define the model \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nmodel.summary() ","metadata":{"id":"zXef38nBFxir","outputId":"7ae99521-46f8-4c6f-9cba-4979deffeee8","_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-12-05T17:55:35.860419Z","iopub.execute_input":"2023-12-05T17:55:35.860755Z","iopub.status.idle":"2023-12-05T17:55:38.108513Z","shell.execute_reply.started":"2023-12-05T17:55:35.860721Z","shell.execute_reply":"2023-12-05T17:55:38.106696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')","metadata":{"id":"Lwfi1Fm8Fxiz","execution":{"iopub.status.busy":"2023-12-05T17:55:38.110513Z","iopub.execute_input":"2023-12-05T17:55:38.110892Z","iopub.status.idle":"2023-12-05T17:55:38.127876Z","shell.execute_reply.started":"2023-12-05T17:55:38.110853Z","shell.execute_reply":"2023-12-05T17:55:38.124771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('/kaggle/input/usable1/summary.h5')","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:38.129760Z","iopub.execute_input":"2023-12-05T17:55:38.130124Z","iopub.status.idle":"2023-12-05T17:55:38.992467Z","shell.execute_reply.started":"2023-12-05T17:55:38.130091Z","shell.execute_reply":"2023-12-05T17:55:38.989735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reverse_target_word_index=y_tokenizer.index_word\nreverse_source_word_index=x_tokenizer.index_word\ntarget_word_index=y_tokenizer.word_index\n\n# Encode the input sequence to get the feature vector\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs) \n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n#attention inference\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_inf_concat) \n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n    [decoder_outputs2] + [state_h2, state_c2])","metadata":{"id":"9QkrNV-4Fxjt","execution":{"iopub.status.busy":"2023-12-05T17:55:38.993422Z","iopub.status.idle":"2023-12-05T17:55:38.993918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['sostok']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","metadata":{"id":"6f6TTFnBFxj6","execution":{"iopub.status.busy":"2023-12-05T17:55:38.995736Z","iopub.status.idle":"2023-12-05T17:55:38.996397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","metadata":{"id":"aAUntznIFxj9","execution":{"iopub.status.busy":"2023-12-05T17:55:38.997654Z","iopub.status.idle":"2023-12-05T17:55:38.998231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contractions_dict = {\"ain't\": 'am not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"cuz\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he's\": 'he is', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is', \"there'd\": 'there had', \"there'd've\": 'there would have', \"there's\": 'there is', \"they'd\": 'they would', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you will have', \"you're\": 'you are', \"you've\": 'you have', \"i'd\": 'i would', \"i'd've\": 'i would have', \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'm\": 'i am', \"i've\": 'i have', \"this's\": 'this is', \"here's\": 'here is', \"ya'll\": 'you all', 'gonna': 'going to', 'gotta': 'got to', 'wanna': 'want to', 'shoulda': 'should have', 'coulda': 'could have', 'woulda': 'would have', 'mighta': 'might have', 'musta': 'must have', 'oughta': 'ought to', 'dunno': 'do not know', 'kinda': 'kind of', 'sorta': 'sort of', 'gotcha': 'got you', 'gimme': 'give me', 'lemme': 'let me', 'wassup': 'what is up', \"c'mon\": 'come on', 'whatcha': 'what are you', 'ya': 'you', 'hafta': 'have to', 'shouldna': 'should not have', 'couldna': 'could not have', 'wouldna': 'would not have', 'mightna': 'might not have', 'mustna': 'must not have', 'oughtna': 'ought not to have', \"amn't\": 'am not'}\ndef replace_contractions(review):\n    for contraction, expanded_form in contractions_dict.items():\n        review = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expanded_form, review, flags=re.IGNORECASE)\n\n    review = re.sub(' +', ' ', review)\n\n    return review\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\ndef clean_review(review):\n    review = review.lower()\n    review = replace_contractions(review)\n    review = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', review, flags=re.MULTILINE)\n    review = re.sub(r'\\<a href', ' ', review)\n    review = re.sub(r'&amp;', '', review) \n    review = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', review)\n    review = re.sub(r'<br />', ' ', review)\n    review = re.sub(r'\\'', ' ', review)\n    \n    words = word_tokenize(review)\n    filtered_words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n    cleaned_review = ' '.join(filtered_words)\n\n    return cleaned_review","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:38.999820Z","iopub.status.idle":"2023-12-05T17:55:39.000401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summarize(custom_review):\n    cleaned_review = clean_review(custom_review)\n    tokenized_review = x_tokenizer.texts_to_sequences([cleaned_review])\n\n    # Step 2: Pad the sequence to match the required input length\n    padded_review = pad_sequences(tokenized_review, maxlen=max_text_len, padding='post')\n\n    # Step 3: Use the trained model for inference\n    # Note: Make sure your model variable is loaded with the trained model\n    summary = decode_sequence(padded_review)\n\n    # Print the result\n    print(\"Original Review:\", custom_review)\n    print(\"Predicted Summary:\", summary)\n    print(\"\\n\\n\")\n    return summary.strip()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.001745Z","iopub.status.idle":"2023-12-05T17:55:39.002314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_reviews(product, sentiment):\n    reviews = []\n    df = pd.read_csv('/kaggle/input/evaluation-dataset/model_evaluation_reviews.csv')\n    for index, row in df.iterrows():\n        if row['Sentiment'] == sentiment and row['Item Name'] == product:\n            reviews.append(row['Review'])\n    return reviews","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.003563Z","iopub.status.idle":"2023-12-05T17:55:39.004171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind_reviews = get_reviews('Pub Mix', 1)\nlen(ind_reviews)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.005464Z","iopub.status.idle":"2023-12-05T17:55:39.006054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npredicted_summaries = []\nfor r in ind_reviews:\n    predicted_summaries.append(summarize(r))","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.007224Z","iopub.status.idle":"2023-12-05T17:55:39.007779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import TruncatedSVD\nimport matplotlib.pyplot as plt\n\n# Create a TF-IDF matrix for the summaries\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(predicted_summaries)\n\n# Reduce dimensionality for visualization\nlsa = TruncatedSVD(n_components=2)\nX_reduced = lsa.fit_transform(X)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.01, min_samples=2)\npredicted_clusters = dbscan.fit_predict(X_reduced)\n\n# Print the predicted clusters\nprint(\"Predicted Clusters:\")\ncluster_info = []\nfor cluster_id in set(predicted_clusters):\n    cluster_indices = [i for i, cluster in enumerate(predicted_clusters) if cluster == cluster_id]\n    cluster_summaries = [predicted_summaries[i] for i in cluster_indices]\n    \n    cluster_info.append({\n        \"Cluster\": cluster_id,\n        \"Summaries\": cluster_summaries\n    })\n\n    print(f\"Cluster {cluster_id}: {', '.join(cluster_summaries)}\")\n\n# Plot the clusters\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=predicted_clusters, cmap='viridis')\nplt.title('DBSCAN Clustering of Predicted Summaries')\nplt.show()\n\n# Print the 2D list of clusters\nprint(\"\\n2D List of Clusters:\")\nfor info in cluster_info:\n    print(f\"Cluster {info['Cluster']}: {info['Summaries']}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.008785Z","iopub.status.idle":"2023-12-05T17:55:39.009345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_summaries = set()\n\nfor info in cluster_info:\n    summary = summarize(\" \".join(info['Summaries']))\n    final_summaries.add(summary)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.011149Z","iopub.status.idle":"2023-12-05T17:55:39.011722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(final_summaries)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.012849Z","iopub.status.idle":"2023-12-05T17:55:39.013417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tags_to_sentence(tags):\n    sentence_template = \"Customers like this product due to its \"\n    sentence = sentence_template + \", \".join(tags)\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.014710Z","iopub.status.idle":"2023-12-05T17:55:39.015323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_sentence_summary = tags_to_sentence(list(final_summaries))\nfinal_sentence_summary","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.016377Z","iopub.status.idle":"2023-12-05T17:55:39.016960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\n\ndef calculate_similarity(sentence1, sentence2):\n    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n\n    embeddings = embed([sentence1, sentence2])\n    similarity = np.inner(embeddings, embeddings)[0, 1]\n    return similarity\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.018110Z","iopub.status.idle":"2023-12-05T17:55:39.019185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amazon_ai_summary = \"Customers like the taste, quality, value, and texture of the snack mix. They mention that the flavors are good, the texture is crunchy and savory, and the size is a great value.\"\namazon_ai_tags = [\"good taste\", \"good quality\", \"good value\", \"good texture\", \"good size\", \"good packaging\"]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.020909Z","iopub.status.idle":"2023-12-05T17:55:39.021535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_similarity(amazon_ai_summary, final_sentence_summary)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.022668Z","iopub.status.idle":"2023-12-05T17:55:39.023258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ai_tag_sentence = tags_to_sentence(amazon_ai_tags)\nai_tag_sentence","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.024341Z","iopub.status.idle":"2023-12-05T17:55:39.024954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_similarity(ai_tag_sentence, final_sentence_summary)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:55:39.025928Z","iopub.status.idle":"2023-12-05T17:55:39.026495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}